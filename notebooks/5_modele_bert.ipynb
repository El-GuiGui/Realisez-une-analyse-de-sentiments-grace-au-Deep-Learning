{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué. Error loading \"c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     AutoTokenizer,\n\u001b[32m     13\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m     14\u001b[39m     Trainer,\n\u001b[32m     15\u001b[39m     TrainingArguments,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué. Error loading \"c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_PATH = ROOT / \"data\"\n",
    "OUT_PATH = ROOT / \"out\"\n",
    "SCRIPTS_PATH = ROOT / \"scripts\"\n",
    "\n",
    "sys.path.append(str(SCRIPTS_PATH))\n",
    "\n",
    "from preprocessing import preprocess_bert\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing (BERT)\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file:{ROOT / 'mlruns'}\")\n",
    "mlflow.set_experiment(\"sentiment_airparadis_bert\")\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   row_id  split\n",
       " 0       0  train\n",
       " 1       1  train\n",
       " 2       2  train\n",
       " 3       3   test\n",
       " 4       4  train,\n",
       " 1527316)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH / \"training.1600000.processed.noemoticon.csv\",\n",
    "    encoding=\"latin-1\",\n",
    "    header=None,\n",
    "    names=col_names,\n",
    ")\n",
    "\n",
    "df[\"label\"] = (df[\"target\"] == 4).astype(int)\n",
    "\n",
    "df = df.reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "\n",
    "split = pd.read_csv(OUT_PATH / \"split.csv\")\n",
    "\n",
    "df = df.merge(split, left_on=\"row_id\", right_on=\"ids\", how=\"inner\")\n",
    "\n",
    "df[[\"row_id\", \"split\"]].head(), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing (BERT): 100%|██████████| 1527316/1527316 [00:09<00:00, 161758.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train réduit à 50000 ModernBERT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 305464)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_bert\"] = df[\"text\"].progress_apply(preprocess_bert)\n",
    "\n",
    "df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "df_test = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "\n",
    "train_sample_size = 50_000\n",
    "\n",
    "if len(df_train) > train_sample_size:\n",
    "    df_train = df_train.sample(train_sample_size, random_state=42)\n",
    "    print(f\"Train réduit à {len(df_train)} ModernBERT.\")\n",
    "else:\n",
    "    print(f\"Train complet utilisé ({len(df_train)} exemples).\")\n",
    "\n",
    "\n",
    "X_train_text = df_train[\"text_bert\"].astype(str).tolist()\n",
    "X_test_text = df_test[\"text_bert\"].astype(str).tolist()\n",
    "y_train = df_train[\"label\"].values\n",
    "y_test = df_test[\"label\"].values\n",
    "\n",
    "len(X_train_text), len(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:02<00:00, 16868.68 examples/s]\n",
      "Map: 100%|██████████| 305464/305464 [00:14<00:00, 20766.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test réduit à 50000 exemples pour ModernBERT (sur 305464).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': tensor(1),\n",
       " 'input_ids': tensor([50281,  1147,   434,  9902,   272,   751,   330,  5214,    59,    58,\n",
       "           987,  1024,     2,   309,  2389,   352,  1969,   816,  3524,   627,\n",
       "           403,   642, 15070,  8585,     2, 50282, 50283, 50283, 50283, 50283,\n",
       "         50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
       "         50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
       "         50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
       "         50283, 50283, 50283, 50283]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "max_length = 64\n",
    "\n",
    "train_df_hf = pd.DataFrame({\"text\": X_train_text, \"label\": y_train})\n",
    "test_df_hf = pd.DataFrame({\"text\": X_test_text, \"label\": y_test})\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df_hf)\n",
    "test_ds = Dataset.from_pandas(test_df_hf)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "def prepare_for_torch(ds):\n",
    "    cols_to_remove = [c for c in [\"text\", \"__index_level_0__\"] if c in ds.column_names]\n",
    "    ds = ds.remove_columns(cols_to_remove)\n",
    "    ds.set_format(\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_tokenized = prepare_for_torch(train_tokenized)\n",
    "test_tokenized = prepare_for_torch(test_tokenized)\n",
    "\n",
    "\n",
    "eval_size = 50_000\n",
    "\n",
    "if len(test_tokenized) > eval_size:\n",
    "    rng = np.random.default_rng(42)\n",
    "    indices = rng.choice(len(test_tokenized), size=eval_size, replace=False)\n",
    "    test_eval = test_tokenized.select(indices.tolist())\n",
    "    print(\n",
    "        f\"Test réduit à {len(test_eval)} exemples pour ModernBERT (sur {len(test_tokenized)}).\"\n",
    "    )\n",
    "else:\n",
    "    test_eval = test_tokenized\n",
    "    print(f\"Test complet utilisé ({len(test_eval)} exemples).\")\n",
    "\n",
    "\n",
    "train_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"\n",
    "    )\n",
    "\n",
    "    if logits.shape[1] == 2:\n",
    "        proba_1 = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "        roc_auc = roc_auc_score(labels, proba_1)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gui\\AppData\\Local\\Temp\\ipykernel_17564\\2634806040.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUT_PATH / \"modernbert\"),\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_eval,  # testing (test_tokenized pour les 300k)\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025/12/01 15:39:01 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id 3c865f091fc0404ab81151673c8effa7: Failed to log run data: Exception: Changing param values is not allowed. Param with key='max_length' was already logged with value='64' for run ID='3c865f091fc0404ab81151673c8effa7'. Attempted logging new value '20'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [294/294 4:40:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [98/98 32:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35788649320602417,\n",
       " 'eval_accuracy': 0.84474,\n",
       " 'eval_precision': 0.842803865741669,\n",
       " 'eval_recall': 0.8454143201930813,\n",
       " 'eval_f1': 0.8441070747233769,\n",
       " 'eval_roc_auc': 0.9217781821637926,\n",
       " 'eval_runtime': 1961.3742,\n",
       " 'eval_samples_per_second': 25.492,\n",
       " 'eval_steps_per_second': 0.05,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"modernbert_base\"):\n",
    "\n",
    "    mlflow.log_param(\"model_id\", model_id)\n",
    "    mlflow.log_param(\"max_length\", max_length)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "\n",
    "    for k, v in eval_metrics.items():\n",
    "        if isinstance(v, (int, float, np.floating)):\n",
    "            mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "\n",
    "    save_dir = OUT_PATH / \"modernbert_model\"\n",
    "    trainer.save_model(str(save_dir))\n",
    "    tokenizer.save_pretrained(str(save_dir))\n",
    "\n",
    "    mlflow.log_artifacts(str(save_dir), artifact_path=\"model\")\n",
    "\n",
    "eval_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
