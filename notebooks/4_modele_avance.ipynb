{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 4 : Modèle avancé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/12/01 04:40:25 INFO mlflow.tracking.fluent: Experiment with name 'sentiment_airparadis_modele_avance' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file:{Path('..').resolve() / 'mlruns'}\")\n",
    "mlflow.set_experiment(\"sentiment_airparadis_modele_avance\")\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_PATH = ROOT / \"data\"\n",
    "OUT_PATH = ROOT / \"out\"\n",
    "SCRIPTS_PATH = ROOT / \"scripts\"\n",
    "EMB_PATH = DATA_PATH / \"embeddings\"\n",
    "\n",
    "sys.path.append(str(SCRIPTS_PATH))\n",
    "\n",
    "from preprocessing import preprocess_advanced\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing (advanced)\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   row_id  split\n",
       " 0       0  train\n",
       " 1       1  train\n",
       " 2       2  train\n",
       " 3       3   test\n",
       " 4       4  train,\n",
       " 1527316)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH / \"training.1600000.processed.noemoticon.csv\",\n",
    "    encoding=\"latin-1\",\n",
    "    header=None,\n",
    "    names=col_names,\n",
    ")\n",
    "df[\"label\"] = (df[\"target\"] == 4).astype(int)\n",
    "\n",
    "df = df.reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "\n",
    "split = pd.read_csv(OUT_PATH / \"split.csv\")\n",
    "\n",
    "df = df.merge(split, left_on=\"row_id\", right_on=\"ids\", how=\"inner\")\n",
    "\n",
    "df[[\"row_id\", \"split\"]].head(), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing (advanced): 100%|██████████| 1527316/1527316 [01:46<00:00, 14316.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1221852, 305464)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text_adv\"] = df[\"text\"].progress_apply(preprocess_advanced)\n",
    "\n",
    "df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "df_test = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "X_train_text = df_train[\"text_adv\"].astype(str).tolist()\n",
    "X_test_text = df_test[\"text_adv\"].astype(str).tolist()\n",
    "y_train = df_train[\"label\"].values\n",
    "y_test = df_test[\"label\"].values\n",
    "\n",
    "len(X_train_text), len(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len choisi : 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1221852, 23), (305464, 23))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 50_000\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "lengths = [len(seq) for seq in X_train_seq]\n",
    "max_len = int(np.percentile(lengths, 95))\n",
    "print(\"max_len choisi :\", max_len)\n",
    "\n",
    "X_train_pad = pad_sequences(\n",
    "    X_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "X_test_pad = pad_sequences(\n",
    "    X_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "X_train_pad.shape, X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_txt(path, embedding_dim, has_header=False):\n",
    "    embeddings_index = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        if has_header:\n",
    "            next(f)\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(\" \")\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if len(coefs) != embedding_dim:\n",
    "                continue\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Embeddings chargés depuis {path} : {len(embeddings_index)} mots\")\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def build_embedding_matrix(tokenizer, embeddings_index, max_words, embedding_dim):\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_words, len(word_index) + 1)\n",
    "\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim), dtype=\"float32\")\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_words:\n",
    "            continue\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "    print(\"Matrice d'embedding shape :\", embedding_matrix.shape)\n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "\n",
    "def build_lstm_model(\n",
    "    num_words,\n",
    "    embedding_dim,\n",
    "    embedding_matrix,\n",
    "    max_len,\n",
    "    lstm_units=128,\n",
    "    dropout_rate=0.3,\n",
    "    bidirectional=False,\n",
    "    trainable=False,\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=num_words,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=trainable,\n",
    "        )\n",
    "    )\n",
    "    if bidirectional:\n",
    "        model.add(Bidirectional(LSTM(lstm_units)))\n",
    "    else:\n",
    "        model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings chargés depuis C:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\data\\embeddings\\glove.twitter.27B.200d.txt : 1193514 mots\n",
      "Matrice d'embedding shape : (50000, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/12/01 04:43:41 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential' has no defined input shape yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4295/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7855 - loss: 0.4525"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 31ms/step - accuracy: 0.8041 - loss: 0.4249 - val_accuracy: 0.7744 - val_loss: 0.4765\n",
      "Epoch 2/5\n",
      "\u001b[1m4295/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8250 - loss: 0.3898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 31ms/step - accuracy: 0.8262 - loss: 0.3868 - val_accuracy: 0.7940 - val_loss: 0.4484\n",
      "Epoch 3/5\n",
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 31ms/step - accuracy: 0.8350 - loss: 0.3702 - val_accuracy: 0.7574 - val_loss: 0.5216\n",
      "Epoch 4/5\n",
      "\u001b[1m4295/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8417 - loss: 0.3581"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 31ms/step - accuracy: 0.8412 - loss: 0.3582 - val_accuracy: 0.8050 - val_loss: 0.4208\n",
      "Epoch 5/5\n",
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 31ms/step - accuracy: 0.8466 - loss: 0.3482 - val_accuracy: 0.7838 - val_loss: 0.4653\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/01 04:54:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9546/9546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step\n",
      "GloVe - accuracy : 0.8304055469711652  | F1 : 0.8265064517965566  | ROC AUC : 0.9116228564071932\n"
     ]
    }
   ],
   "source": [
    "glove_path = EMB_PATH / \"glove.twitter.27B.200d.txt\"\n",
    "embedding_dim_glove = 200\n",
    "\n",
    "emb_index_glove = load_embeddings_txt(glove_path, embedding_dim_glove, has_header=False)\n",
    "embedding_matrix_glove, num_words_glove = build_embedding_matrix(\n",
    "    tokenizer, emb_index_glove, max_words, embedding_dim_glove\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"lstm_glove\"):\n",
    "\n",
    "    mlflow.log_param(\"embedding_type\", \"glove_twitter_100d\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim_glove)\n",
    "    mlflow.log_param(\"max_words\", max_words)\n",
    "    mlflow.log_param(\"max_len\", max_len)\n",
    "    mlflow.log_param(\"lstm_units\", 128)\n",
    "    mlflow.log_param(\"bidirectional\", False)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "    model_glove = build_lstm_model(\n",
    "        num_words=num_words_glove,\n",
    "        embedding_dim=embedding_dim_glove,\n",
    "        embedding_matrix=embedding_matrix_glove,\n",
    "        max_len=max_len,\n",
    "        lstm_units=128,\n",
    "        dropout_rate=0.3,\n",
    "        bidirectional=False,\n",
    "        trainable=False,\n",
    "    )\n",
    "\n",
    "    history = model_glove.fit(\n",
    "        X_train_pad,\n",
    "        y_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_proba = model_glove.predict(X_test_pad).ravel()\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"binary\"\n",
    "    )\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", acc)\n",
    "    mlflow.log_metric(\"test_precision\", precision)\n",
    "    mlflow.log_metric(\"test_recall\", recall)\n",
    "    mlflow.log_metric(\"test_f1\", f1)\n",
    "    mlflow.log_metric(\"test_roc_auc\", roc_auc)\n",
    "\n",
    "print(\"GloVe - accuracy :\", acc, \" | F1 :\", f1, \" | ROC AUC :\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\env2\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025/12/01 04:56:43 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: Sequential model 'sequential_1' has no defined input shape yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings chargés depuis C:\\Users\\Gui\\Desktop\\AAA_doc\\Openclassroom school\\Python project\\proj_proj\\proj7\\data\\embeddings\\wiki-news-300d-1M-subword.vec : 999994 mots\n",
      "Matrice d'embedding shape : (50000, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4295/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7481 - loss: 0.5083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 44ms/step - accuracy: 0.7712 - loss: 0.4769 - val_accuracy: 0.7153 - val_loss: 0.5761\n",
      "Epoch 2/5\n",
      "\u001b[1m4295/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7994 - loss: 0.4332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 44ms/step - accuracy: 0.8019 - loss: 0.4288 - val_accuracy: 0.7378 - val_loss: 0.5370\n",
      "Epoch 3/5\n",
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 44ms/step - accuracy: 0.8110 - loss: 0.4128 - val_accuracy: 0.7360 - val_loss: 0.5437\n",
      "Epoch 4/5\n",
      "\u001b[1m4295/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8164 - loss: 0.4031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 44ms/step - accuracy: 0.8169 - loss: 0.4020 - val_accuracy: 0.7422 - val_loss: 0.5324\n",
      "Epoch 5/5\n",
      "\u001b[1m4296/4296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 44ms/step - accuracy: 0.8222 - loss: 0.3931 - val_accuracy: 0.6882 - val_loss: 0.6277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/01 05:12:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9546/9546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 4ms/step\n",
      "fastText - accuracy : 0.8104784851897441  | F1 : 0.7973423322504761  | ROC AUC : 0.8973571987590773\n"
     ]
    }
   ],
   "source": [
    "fasttext_path = EMB_PATH / \"wiki-news-300d-1M-subword.vec\"\n",
    "embedding_dim_ft = 300\n",
    "\n",
    "emb_index_ft = load_embeddings_txt(fasttext_path, embedding_dim_ft, has_header=True)\n",
    "embedding_matrix_ft, num_words_ft = build_embedding_matrix(\n",
    "    tokenizer, emb_index_ft, max_words, embedding_dim_ft\n",
    ")\n",
    "\n",
    "batch_size_ft = 256\n",
    "epochs_ft = 5\n",
    "\n",
    "es_ft = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"bilstm_fasttext\"):\n",
    "\n",
    "    mlflow.log_param(\"embedding_type\", \"fasttext_cc_300d\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim_ft)\n",
    "    mlflow.log_param(\"max_words\", max_words)\n",
    "    mlflow.log_param(\"max_len\", max_len)\n",
    "    mlflow.log_param(\"lstm_units\", 128)\n",
    "    mlflow.log_param(\"bidirectional\", True)\n",
    "    mlflow.log_param(\"batch_size\", batch_size_ft)\n",
    "    mlflow.log_param(\"epochs\", epochs_ft)\n",
    "\n",
    "    model_ft = build_lstm_model(\n",
    "        num_words=num_words_ft,\n",
    "        embedding_dim=embedding_dim_ft,\n",
    "        embedding_matrix=embedding_matrix_ft,\n",
    "        max_len=max_len,\n",
    "        lstm_units=128,\n",
    "        dropout_rate=0.3,\n",
    "        bidirectional=True,\n",
    "        trainable=False,\n",
    "    )\n",
    "\n",
    "    history_ft = model_ft.fit(\n",
    "        X_train_pad,\n",
    "        y_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=epochs_ft,\n",
    "        batch_size=batch_size_ft,\n",
    "        callbacks=[es_ft],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_proba_ft = model_ft.predict(X_test_pad).ravel()\n",
    "    y_pred_ft = (y_proba_ft >= 0.5).astype(int)\n",
    "\n",
    "    acc_ft = accuracy_score(y_test, y_pred_ft)\n",
    "    precision_ft, recall_ft, f1_ft, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred_ft, average=\"binary\"\n",
    "    )\n",
    "    roc_auc_ft = roc_auc_score(y_test, y_proba_ft)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", acc_ft)\n",
    "    mlflow.log_metric(\"test_precision\", precision_ft)\n",
    "    mlflow.log_metric(\"test_recall\", recall_ft)\n",
    "    mlflow.log_metric(\"test_f1\", f1_ft)\n",
    "    mlflow.log_metric(\"test_roc_auc\", roc_auc_ft)\n",
    "\n",
    "print(\"fastText - accuracy :\", acc_ft, \" | F1 :\", f1_ft, \" | ROC AUC :\", roc_auc_ft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
